#+TITLE: Main file of the phage repo
#+AUTHOR: Hugo Ávila (@bioinformagica)
#+LANGUAGE: en-us
#+STARTUP: overview
#+PROPERTY: header-args :dir ~/projects/phage-evo-paper :mkdirp yes :exports none :eval never-export

* ENV setup
** Snakemake
#+BEGIN_SRC shell
# get conda installer
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# Run installer and interactively accept and init the conda executable
# Atention !!!: this will change your current shell .rc (.zshrc, .bashrc ...)
bash Miniconda3-latest-Linux-x86_64.sh

# Source the conda changes
source ~/.bashrc

# Set auto activation of conda base environment to false
conda config --set auto_activate_base false

# Add channels
conda config --add channels conda-forge
conda config --add channels bioconda

# Install mamba
conda install -n base -c conda-forge mamba -y

# Install Snakemake
mamba create -c conda-forge -c bioconda -n snakemake snakemake -y

#+END_SRC

#+RESULTS:

* Snakemake workflow template
#+NAME: cb:get-snakemake-template
#+CAPTION: Clone the Snakemake Template
#+BEGIN_SRC shell
git clone https://github.com/snakemake-workflows/snakemake-workflow-template .
rm -rf .git
git init && git commit --allow-empty -m "Initial Commit."
#+END_SRC

* README.md
#+NAME: cb:README.md
#+CAPTION: README.md
#+BEGIN_SRC markdown :tangle README.md
# Snakemake workflow: `phage-evo-paper`

[![Snakemake](https://img.shields.io/badge/snakemake-≥6.3.0-brightgreen.svg)](https://snakemake.github.io)

A Snakemake workflow for `Phage directed evolution analysis`.
#+END_SRC
** DONE Create README.md
** TODO Create linter action
* Snakefile
:PROPERTIES:
:COOKIE_DATA: todo recursive
:header-args: :tangle workflow/Snakefile :mkdirp yes :exports none :eval never-export :comments link
:END:
** IMPORTS
#+BEGIN_SRC snakemake
import os
import sys
from operator import itemgetter as itget
import glob
from pathlib import Path
#+END_SRC

** FILE CONFIGS
#+BEGIN_SRC snakemake
configfile: 'config/config.yaml'
#+END_SRC

** FUNCTIONS
#+BEGIN_SRC snakemake
get_cores_perc = lambda perc: max(1, workflow.cores * perc)
join_path = lambda *args: os.path.join(*args)
get_Kbs = lambda x: int(x/1_000)

def get_current_HEAD_hash(default='unnamed'):
    shell_CL = "git show-ref --head --heads --abbrev | grep HEAD | awk '{print $1}'"
    with os.popen(shell_CL) as f:
        git_hash = f.read().strip()
        if re.match('^[\w\d]{7}$', git_hash):
            return git_hash
        return default
#+END_SRC

** GLOBAL VARIABLES
#+BEGIN_SRC snakemake
SAMPLES, = glob_wildcards(join_path(config['data']['reads'], '{sample}' + config['data']['reads_suffix']))
EXPERIMENTS = config['experiments']

snakefile_path = os.path.dirname(workflow.snakefile)
# results_dir = join_path('results', get_current_HEAD_hash())
results_dir = join_path('results', 'without_error_corr')
scripts_dir = join_path(snakefile_path, 'scripts')
#+END_SRC
** MAIN RULE ALL
#+BEGIN_SRC snakemake
rule all:
    input:
        rectangular = expand(join_path(results_dir, 'fastani', '{experiment}', '{experiment}.ggtree.ecoli.phages.passages.rectangular.sample_size_' + str(config['sample_size']) +  '.pdf'), experiment=EXPERIMENTS),
        phanotate_finished = expand(join_path(results_dir, 'annotations', 'phanotate', '{experiment}', 'finished_phanotate'), experiment=EXPERIMENTS),
        prokka_finished = expand(join_path(results_dir, 'annotations', 'prokka', '{experiment}', 'finished_prokka'), experiment=EXPERIMENTS)
#+END_SRC

** Include
#+BEGIN_SRC snakemake
include:
    'rules/processing_all_samples.smk'
#+END_SRC

* Rules
** Processing all experiments
:PROPERTIES:
:COOKIE_DATA: todo recursive
:header-args: :tangle workflow/rules/processing_all_samples.smk :mkdirp yes :exports none :eval never-export :comments link
:END:
*** Plot lengths
#+BEGIN_SRC snakemake
rule quality_check_plot:
    input:
        reads = join_path(config['data']['reads'], '{sample}' + config['data']['reads_suffix'])
    output:
        plot_dir = directory(join_path(results_dir, 'nanoplot', '{sample}'))
    threads:
        4
    conda:
        "../envs/nanoplot_env.yaml"
    shell:
        "NanoPlot -t {threads} --plots dot --fastq {input.reads} -o {output.plot_dir}"
#+END_SRC
*** Extract phage genomes
#+BEGIN_SRC snakemake
rule filter_by_lenght_to_get_phage_genomes:
    input:
        reads = join_path(config['data']['reads'], '{sample}' + config['data']['reads_suffix']),
    output:
        putative_phage_genomes = join_path(results_dir, 'minia', '{sample}', '{sample}' + '.putative_phage_genomes' + '.fasta'),
    params:
        ,**config['params']['seqkit'],
    threads:
        1
    conda:
        '../envs/graphaligner_env.yaml'
    shell:
        'seqkit seq {input.reads} -j {threads} -m {params.min} -M {params.max} | seqkit fq2fa | '
        "sed -r '/>/ s|>|>{wildcards.sample}#1#|' > {output.putative_phage_genomes} "
#+END_SRC

*** Filter genomes
#+BEGIN_SRC snakemake
rule filter_out_bacterial_genomes:
    input:
        target = config['data']['genomes']['ecoli_and_phages'],
        putative_phage_genomes = expand(join_path(results_dir, 'minia', '{sample}', '{sample}' + '.putative_phage_genomes' + '.fasta'), sample=SAMPLES),
    output:
        all_genomes_merged = join_path(results_dir, 'pggb', 'all_genomes_merged.fa.gz'),
        all_genomes_merged_filtered = join_path(results_dir, 'pggb', 'all_genomes_merged.filter_out_bacteria.fa.gz'),
        ids_to_keep = join_path(results_dir, 'pggb', 'ids_to_keep.txt'),
    params:
        ,**config['params']['removing_bacteria'],
    threads:
        get_cores_perc(1)
    conda:
        '../envs/pggb_env.yaml'
    shell:
        "cat {input.putative_phage_genomes} | bgzip -@ {threads} >{output.all_genomes_merged} && "
        "samtools faidx {output.all_genomes_merged} && "
        "samtools faidx {output.all_genomes_merged} "
        "-r <(wfmash {input.target} {output.all_genomes_merged} -s {params.segment_length} -l {params.block_length} -p {params.map_pct_id} -t {threads} | "
        "awk -v min_qcov={params.min_qcov} '/E_coli/ {{ qcov=$11/$2; if ( !(qcov >= min_qcov) ) print $1; }}' | sort -u | tee {output.ids_to_keep} ) > "
        "{output.all_genomes_merged_filtered}"
#+END_SRC

*** Sample genomes
#+BEGIN_SRC snakemake
rule sample_genomes:
    input:
        all_genomes_merged_filtered = join_path(results_dir, 'pggb', 'all_genomes_merged.filter_out_bacteria.fa.gz'),
        ids_to_keep = join_path(results_dir, 'pggb', 'ids_to_keep.txt'),
        codes = join_path('data', 'tables', 'codes.tsv'),
    output:
        pggb_input = join_path(results_dir, 'pggb', '{experiment}', '{experiment}.merged_genomes.sample_size_' + str(config['sample_size']) + '.fa.gz'),
        sample_ids = join_path(results_dir, 'pggb', '{experiment}', '{experiment}.ids.sample_size_' + str(config['sample_size']) + '.txt'),
    params:
        sample_size = config['sample_size'],
        log_dir = join_path(str(Path('results').parent.absolute()), 'logs'),
    threads:
        get_cores_perc(1)
    conda:
        '../envs/pggb_env.yaml'
    shell:
        'exec &> >( tee {params.log_dir}/{rule}_{wildcards.experiment}_$(date +%Y_%m_%d_-_%H_%M_%S).log ) && '
        "awk -F$'\\t' '/^{wildcards.experiment}/ {{print $3}}' {input.codes}  | "
        'while read f; do grep -P "^${{f}}#" {input.ids_to_keep} | shuf -n {params.sample_size}; done | tee {output.sample_ids} && '
        "samtools faidx {input.all_genomes_merged_filtered} -r {output.sample_ids} | "
        'bgzip -@ {threads} > {output.pggb_input} '
#+END_SRC
*** Fastani
#+BEGIN_SRC snakemake
rule fastaANI_distance_matrix:
    input:
        pggb_input = join_path(results_dir, 'pggb', '{experiment}', '{experiment}.merged_genomes.sample_size_' + str(config['sample_size']) + '.fa.gz'),
    output:
        split_fastas = directory(join_path(results_dir, 'fastani',  '{experiment}', '{experiment}.split_fasta_' + str(config['sample_size']) )),
        fastani_distance_matrix = join_path(results_dir, 'fastani', '{experiment}', '{experiment}.fastani_distance_matrix.sample_size_' + str(config['sample_size']) + '.tsv'),
        list_of_files = join_path(results_dir, 'fastani', '{experiment}', '{experiment}.list_of_splited_fastas_pahts.sample_size_' + str(config['sample_size']) + '.txt'),
    params:
        ,**config['params']['fastani'],
        log_dir = join_path(str(Path('results').parent.absolute()), 'logs')
    conda:
        '../envs/fastani_env.yaml'
    threads:
        get_cores_perc(1)
    shell:
        'exec &> >( tee {params.log_dir}/{rule}_{wildcards.experiment}_$(date +%Y_%m_%d_-_%H_%M_%S).log ) && '
        'seqkit split -O {output.split_fastas} --by-id {input.pggb_input} && '
        'gunzip {output.split_fastas}/*.fa.gz && '
        "find {output.split_fastas} -name '*.fa' -exec readlink -f {{}} \; > {output.list_of_files} && "
        'fastANI -t {threads} --fragLen {params.frag_lenght} --ql {output.list_of_files} --rl {output.list_of_files} -o /dev/stdout  | '
        "perl -pe 's|/.*?id_||g;s|.fa||g' | awk -v OFS='\\t' '{{print $1,$2,$3}}' >{output.fastani_distance_matrix}"
#+END_SRC
*** Plot FASTANI
#+BEGIN_SRC snakemake
rule plot_fast_ani:
    input:
        fastani_distance_matrix = join_path(results_dir, 'fastani', '{experiment}', '{experiment}.fastani_distance_matrix.sample_size_' + str(config['sample_size']) + '.tsv'),
        codes = join_path('data', 'tables', 'codes.tsv'),
        script_fix_id = join_path(snakefile_path, 'scripts', 'fix_ids.py'),
        script_phylogeny_fastani = join_path(snakefile_path, 'scripts', 'phylogeny_fastani.R'),
    output:
        fastani_distance_matrix_id_fixed = join_path(results_dir, 'fastani', '{experiment}', '{experiment}.fastani_distance_matrix.sample_size_' + str(config['sample_size']) + '.ids_fixed.tsv'),
        rectangular = join_path(results_dir, 'fastani', '{experiment}', '{experiment}.ggtree.ecoli.phages.passages.rectangular.sample_size_' + str(config['sample_size']) +  '.pdf'),
    params:
        title = "{}.sample_size_{}.K{}.A{}_bp_relative.min40K.max50.GA_polished".format('{experiment}', config['sample_size'], config['params']['minia']['kmer'], config['params']['minia']['P1_abundance'])
    conda:
        '../envs/R_env.yaml'
    shell:
        'python3 {input.script_fix_id} {input.fastani_distance_matrix} {input.codes} > {output.fastani_distance_matrix_id_fixed} && '
        'Rscript {input.script_phylogeny_fastani} {output.fastani_distance_matrix_id_fixed} {input.codes} {output.rectangular} {params.title}'
#+END_SRC
*** ORF prediction PHANOTATE
#+BEGIN_SRC snakemake
rule orf_prediction_phanotate:
    input:
        list_of_files = join_path(results_dir, 'fastani', '{experiment}', '{experiment}.list_of_splited_fastas_pahts.sample_size_' + str(config['sample_size']) + '.txt'),
        phanotate_runner = join_path(scripts_dir, 'phanotate_runner.py'),
    output:
        phanotate_dir = directory(join_path(results_dir, 'annotations', 'phanotate', '{experiment}')),
        finished = join_path(results_dir, 'annotations', 'phanotate', '{experiment}', 'finished_phanotate'),
    params:
        ,**config['params']['phanotate'],
        log_dir = join_path(snakefile_path, '..', 'logs'),
    conda:
        '../envs/phanotate_env.yaml'
    threads:
        get_cores_perc(1)
    shell:
        'exec &> >( tee {params.log_dir}/{rule}_{wildcards.experiment}_$(date +%Y_%m_%d_-_%H_%M_%S).log ) && '
        'python3 {input.phanotate_runner} --input_file_list {input.list_of_files} '
        ' --threads {threads} --out_format {params.out_format} --output_dir {output.phanotate_dir} && '
        '>{output.finished} '
#+END_SRC
*** PROKKA annotation
#+BEGIN_SRC snakemake
rule annotation_prokka:
    input:
        list_of_files = join_path(results_dir, 'fastani', '{experiment}', '{experiment}.list_of_splited_fastas_pahts.sample_size_' + str(config['sample_size']) + '.txt'),
        mmseqs_phrogs_db = join_path('data', 'phrogs_db', 'phrogs_rep_seq.fasta'),
        prokka_runner = join_path(scripts_dir, 'prokka_runner.py'),
    output:
        prokka_dir = directory(join_path(results_dir, 'annotations', 'prokka', '{experiment}')),
        finished = join_path(results_dir, 'annotations', 'prokka', '{experiment}', 'finished_prokka'),
    params:
        log_dir = join_path(snakefile_path, '..', 'logs'),
    conda:
        '../envs/prokka_env.yaml'
    threads:
        get_cores_perc(1)
    shell:
        'exec &> >( tee {params.log_dir}/{rule}_{wildcards.experiment}_$(date +%Y_%m_%d_-_%H_%M_%S).log ) && '
        'python3 {input.prokka_runner} --input_file_list {input.list_of_files} '
        '--output_dir {output.prokka_dir} --proteins {input.mmseqs_phrogs_db} '
        '--threads {threads} --prokka_threads 4 && '
        'touch {output.finished}'
#+END_SRC

*** Download prokka proteins db (Phrogs)
#+BEGIN_SRC snakemake
rule download_phrogs_database:
    output:
        phrogs_tar = join_path('data', 'phrogs_db', 'FAA_phrog.tar.gz'),
    params:
        log_dir = join_path(snakefile_path, '..', 'logs'),
        mmseqs_phrogs_url = config['mmseqs_phrogs_url'],
    threads:
        1
    shell:
        'exec &> >( tee {params.log_dir}/{rule}_$(date +%Y_%m_%d_-_%H_%M_%S).log ) && '
        'wget -O {output.phrogs_tar} {params.mmseqs_phrogs_url}'
#+END_SRC

*** Cluster prokka proteins db (Phrogs)
#+BEGIN_SRC snakemake
rule reclust_phrogs_database:
    input:
        phrogs_tar = join_path('data', 'phrogs_db', 'FAA_phrog.tar.gz'),
        format_phrogs_headers = join_path(scripts_dir, 'format_phrogs_headers.py'),
    output:
        mmseqs_phrogs_db = join_path('data', 'phrogs_db', 'phrogs_rep_seq.fasta'),
    params:
        log_dir = join_path(snakefile_path, '..', 'logs'),
        mmseqs_phrogs_url = config['mmseqs_phrogs_url'],
        mmseqs_multifasta_dir = join_path('data', 'phrogs_db', 'FAA_phrog'),
        phrogs_db_dir = join_path('data', 'phrogs_db')
    threads:
        get_cores_perc(1)
    conda:
        '../envs/mmseqs2_env.yaml'
    shell:
        'exec &> >( tee {params.log_dir}/{rule}_$(date +%Y_%m_%d_-_%H_%M_%S).log ) && '
        'tar -xf {input.phrogs_tar} -C {params.phrogs_db_dir} && '
        'cat {params.mmseqs_multifasta_dir}/*.faa | python3 {input.format_phrogs_headers} '
        '> {params.phrogs_db_dir}/multifasta.faa && '
        'mmseqs easy-cluster {params.phrogs_db_dir}/multifasta.faa {params.phrogs_db_dir}/phrogs {params.phrogs_db_dir}/tmp --threads {threads}'
#+END_SRC

** saving some unused rules
*** PGGB
#+BEGIN_SRC snakemake
rule pggb_pangenome:
    input:
        pggb_input = join_path(results_dir, 'pggb', 'genomes.sample_' + str(config['sample_size']) + '_from_each_passage.fa.gz')
    output:
        pggb_out = directory(join_path(results_dir, 'pggb', 'sample_' + str(config['sample_size']) ))
    params:
        ,**config['params']['pggb']
    threads:
        get_cores_perc(1)
    conda:
        '../envs/pggb_env.yaml'
    shell:
        "n_mappings=$( zgrep -c '>' {input.pggb_input} ) && "
        "pggb -m -p {params.map_pct_id} -n $n_mappings -s {params.segment_length} -l {params.block_length} -k {params.min_match_len} -B {params.transclose_batch} -t {threads} -o {output.pggb_out} -i {input.pggb_input}"
#+END_SRC
*** odgi distance matrix
#+BEGIN_SRC snakemake
rule get_distance_metrics:
    input:
        pggb_out = join_path(results_dir, 'pggb', 'sample_' + str(config['sample_size']) )
    output:
        distance_tsv = join_path(results_dir, 'pggb', 'distance_matrix.sample.' + str(config['sample_size']) + '.tsv' )
    threads:
        get_cores_perc(1)
    conda:
        '../envs/pggb_env.yaml'
    shell:
        "odgi paths -t {threads} -d -i {input.pggb_out}/*.smooth.final.og > {output.distance_tsv}"
#+END_SRC
*** Assembly
#+BEGIN_SRC snakemake
rule minia_assembly:
    input:
        reads = join_path(config['data']['reads'], '{sample}' + config['data']['reads_suffix']),
        script_abundance = join_path(snakefile_path, 'scripts', 'get_abundance.sh'),
        script_fa_to_gfa = join_path(snakefile_path, 'scripts', 'convertToGFA.py'),
    output:
        minia_assembly = join_path(results_dir, 'minia', '{sample}', '{sample}.minia.contigs.fa'),
        minia_assembly_gfa = join_path(results_dir, 'minia', '{sample}', '{sample}.minia.contigs.gfa'),
        log_abundance = join_path('logs', '{sample}.abundance.txt'),
    params:
        **config['params']['minia'],
    threads:
        get_cores_perc(0.1)
    conda:
        '../envs/minia_env.yaml'
    shell:
        "RELATIVE_ABUNDACE=$( {input.script_abundance} {params.P1_abundance} {params.P1_bp} {input.reads} ) && "
        'echo "{wildcards.sample},${{RELATIVE_ABUNDACE}}" > {output.log_abundance} && '
        "minia -nb-cores {threads} -kmer-size {params.kmer} -abundance-min $RELATIVE_ABUNDACE "
        "-out $(echo {output.minia_assembly} | sed 's/.contigs.fa//') -in {input.reads} && "
        "find $( dirname {output.minia_assembly} ) -type f ! -name '*'$(basename {output.minia_assembly}) -exec rm {{}} \; && "
        "python {input.script_fa_to_gfa} {output.minia_assembly} {output.minia_assembly_gfa} {params.kmer}"
#+END_SRC
*** Error correction
#+BEGIN_SRC snakemake
rule graphaligner_error_correction:
    input:
        reads = join_path(config['data']['reads'], '{sample}' + config['data']['reads_suffix']),
        minia_assembly_gfa = join_path(results_dir, 'minia', '{sample}', '{sample}.minia.contigs.gfa'),
    output:
        putative_phage_genomes = join_path(results_dir, 'minia', '{sample}', '{sample}' + '.putative_phage_genomes' + '.fastq'),
        putative_phage_genomes_polished = join_path(results_dir, 'minia', '{sample}', '{sample}' + '.putative_phage_genomes' + '.polished' + '.prefixed' + '.fa.gz'),
    params:
        gam = join_path(results_dir, 'minia', '{sample}', '{sample}' + '.putative_phage_genomes' + '.polished' + '.gam'),
        fasta = join_path(results_dir, 'minia', '{sample}', '{sample}' + '.putative_phage_genomes' + '.polished' + '.fa'),
        **config['params']['seqkit'],
        **config['params']['graphaligner'],
    threads:
        get_cores_perc(0.3)
    conda:
        '../envs/graphaligner_env.yaml'
    shell:
        "seqkit seq {input.reads} -j {threads} -m {params.min} -M {params.max} > {output.putative_phage_genomes} && "
        "GraphAligner -g {input.minia_assembly_gfa} -f {output.putative_phage_genomes} -x {params.dbtype} "
        "--threads {threads} --seeds-minimizer-length {params.seed_minimizer} "
        "--seeds-minimizer-windowsize {params.seed_minimizer} -a {params.gam} "
        "--corrected-out {params.fasta} && "
        "sed -r '/>/ s|>|>{wildcards.sample}#1#|;s|\s.+||' {params.fasta} | bgzip > {output.putative_phage_genomes_polished} && "
        "rm {params.gam} {params.fasta}"
#+END_SRC
* CONFIGS
:PROPERTIES:
:COOKIE_DATA: todo recursive
:header-args: :tangle config/config.yaml :mkdirp yes :exports none :eval never-export :comments link
:END:
#+BEGIN_SRC yaml
experiments:
  - 'TREE_1'
  - 'TREE_2'
  - 'TREE_3'
  - 'TREE_4'
  - 'TREE_5'
  - 'TREE_6'

sample_genomes: 'true'
sample_size: 3
tree_title: ''

mmseqs_phrogs_url:
  'https://phrogs.lmge.uca.fr/downloads_from_website/FAA_phrog.tar.gz'

data:
  reads: '/export/erikg/data/phage/reads'
  reads_suffix: '.merged.fastq.gz'
  genomes:
    ecoli: 'data/genomes/bacteria/E_coli_bl21_noplasmid.fasta'
    merged: 'data/genomes/bacteria/parental_phages_and_Ecoli_bl21.fasta'
    bacteira: 'data/genomes/bacteria'
    phages: 'data/genomes/phage'
    ecoli_and_phages: 'data/genomes/ecoli_bl21_DE_and_phages_merged.fasta.gz'

results:
  nanoplot:
    before: 'results/nanoplot/before_filter'
    after: 'results/nanoplot/after_filter'
  assemblies:
    minia: 'results/assemblies/minia'
    miniasm: 'results/assemblies/miniasm'

params:
  minia:
    kmer: 33
    # abundance: 4
    min_contig_lenght: 40_000
    max_contig_lenght: 50_000
    P1_abundance: 5
    P1_bp: 186778684
    cores: 10
  filtlong:
    keep_percent: 90
    min_length: 10_000
  pggb:
    map_pct_id: 95
    segment_length: 500
    block_length: 1_000
    min_match_len: 47
    transclose_batch: 10_000
  seqkit:
    min: 40_000
    max: 50_000
  graphaligner:
    dbtype: 'vg'
    seed_minimizer: 15
  removing_bacteria:
    map_pct_id: 90
    segment_length: 1_000
    block_length: 1_000
    min_qcov: 1
  fastani:
    frag_lenght: 500
  phanotate:
    out_format: 'fasta'
# Sample genomes from assembly
#+END_SRC
* Notes
- Some tools cannot be built through conda only:
  - R: that are some libs that must be isntall manually on the R console:
    - remotes::install_github("YuLab-SMU/ggtree")
    - install.packages("stringi",dep=TRUE)
    - devtools::install_github("tidyverse/tidyr")
  - PGGB: Works better if installed through GUIX
-
** TODO Make a docker container with conda and GUIX
